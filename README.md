# LowLight-Image-Enhancing-UNetGAN
This repo contains the files necessary in order to run the project's U-Net GAN. The results, analysis, and 
generation examples are included in the project report, included as a .pdf file. Further analysis can be run 
sing the `utils` library which includes all the metrics used in the report.

Contained in this repo are also the three models, 
the generator U-Net, a discriminator U-Net, and a small student U-Net which can optionally be trained on 
the generator.

# Requirements
I used conda for dependency management. Use the commands to recreate the environment in a local conda venv.

`conda create --name <env_name>`

`conda activate <env_name>`

`pip install -r requirements.txt`

All necessary packages are noted in `requirements.txt`. Running the full model on 50 epochs of training took ~6 hours on a NVIDIA RTX 3070. 
It is recommended to have access to a GPU to recreate the experiments.

# Dataset Preprocessing
The model requires the SID Dataset from  "Learning to See in the Dark" (Chen et al., 2018). 
Included is a preprocessing script to convert the raw .ARW images into .png images. This can be ran on the terminal 
with `python preprocess.py`. A file called preprocess.yaml is used for the location of the raw dataset and the processed dataset, 
which need to be filled in with the respective folders.

Since the set includes a long and short exposure folder, where long exposures serve as ground truth, the preprocess step should be ran twice, once for each folder. Including both 
in a folder called "Sony_PNG" in the dataset folder is sufficient to run the program. The final dataset folder should look like this:
```
.
├── SIDDataset
│   ├── Sony
│   │   ├── long
|   |   |   ├── <index>_<camera index>_<exposure time>.ARW
│   │   ├── short
|   |   |   ├── <index>_<camera index>_<exposure time>.ARW
│   ├── Sony_PNG
│   │   ├── long
|   |   |   ├── <index>_<camera index>_<exposure time>.png
│   │   ├── short
|   |   |   ├── <index>_<camera index>_<exposure time>.png
|   ├── Sony_test_list.txt
|   ├── Sony_train_list.txt
|   ├── Sony_val_list.txt
.
```

# Running the Program
The model can be trained using `python run.py`. There are various command-line arguments listed below.

| Argument          | Short | Type   | Default | Description                                                                 |
|-------------------|-------|--------|---------|-----------------------------------------------------------------------------|
| `--config`        | `-c`  | String | `config.yaml`  | Path to the input file.                                                     |
| `--resume`        | `-r`  | String | `None` | Path to a .ckpt file to resume from (Lightning format).                                              |
| `--student`       | `-s`  | Flag   | `None` | Flag to activate student training mode.                              |

By default, the model will train purely with no knowledge distillation, but the student flag can be enabled to train both
a teacher and student model. 

Results will be automatically generated by PyTorch Lightning in a folder called `logs`. Included are examples 
from each epoch of the original dark images, ground truth long exposures, and reconstructions from both teacher and student models.



